from pathlib import Path
import hashlib
from .rag_store import RAGStore
from .ataraxai_rag_manager import (
    RAGManifest,
)
from .ataraxai_embedder import AtaraxAIEmbedder
import threading
import os
import queue


# Placeholder for your text chunking function
def chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> list[str]:
    # Implement your chosen chunking strategy here
    # (e.g., RecursiveCharacterTextSplitter, or simpler fixed-size with overlap)
    # This is a very basic example, replace with a robust implementation.
    chunks = []
    start = 0
    if not text:  # Handle empty text
        return []
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        if end >= len(text):
            break
        start += (
            chunk_size - chunk_overlap
        )  # Move start for the next chunk, considering overlap
        if start >= len(
            text
        ):  # Ensure start doesn't go past text length due to overlap
            break
    return chunks


def get_file_hash(file_path: Path) -> str | None:
    sha256_hash = hashlib.sha256()
    try:
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    except IOError as e:
        print(f"Error hashing file {file_path}: {e}")
        return None


def process_new_file(
    file_path_str: str,
    manifest: RAGManifest,
    rag_store: "RAGStore",
    embedder: "AtaraxAIEmbedder",
    chunk_size: int,
    chunk_overlap: int,
):
    file_path = Path(file_path_str)
    print(f"WORKER: Processing NEW file: {file_path}")

    if not file_path.exists() or file_path.is_dir():
        print(f"WORKER: File {file_path} does not exist or is a directory. Skipping.")
        return

    try:
        # 1. Read file content (Add specific file type handling here: PDF, DOCX, etc.)
        # For now, assuming text files for simplicity.
        try:
            file_content = file_path.read_text(encoding="utf-8", errors="replace")
        except Exception as e:
            print(f"WORKER: Error reading file {file_path}: {e}. Skipping.")
            return

        file_timestamp = file_path.stat().st_mtime
        file_hash = get_file_hash(file_path)
        if not file_hash:  # Hashing failed
            return

        # 2. Chunk the content
        text_chunks = chunk_text(file_content, chunk_size, chunk_overlap)
        if not text_chunks:
            print(f"WORKER: No text chunks generated for {file_path}. Skipping.")
            return

        # 3. Prepare data for RAGStore
        chunk_ids = [
            f"{str(file_path)}_{file_hash[:8]}_chunk_{i}"
            for i in range(len(text_chunks))
        ]  # More robust IDs
        metadatas = [
            {
                "source_file": str(file_path),
                "original_filename": file_path.name,
                "chunk_index": i,
                "file_timestamp": file_timestamp,
                "file_hash": file_hash,
            }
            for i in range(len(text_chunks))
        ]

        # 4. Add to RAGStore (which handles embedding via the passed embedder instance)
        print(f"WORKER: Adding {len(text_chunks)} chunks for {file_path} to RAG store.")
        rag_store.add_chunks(
            ids=chunk_ids,
            texts=text_chunks,
            metadatas=metadatas,
            # Embeddings will be generated by rag_store.add_chunks using the provided embedder
        )

        # 5. Update the manifest
        manifest.add_file(
            str(file_path),
            metadata={
                "timestamp": file_timestamp,
                "hash": file_hash,
                "chunk_ids": chunk_ids,
                "status": "indexed",
            },
        )
        manifest.save()
        print(f"WORKER: Successfully processed and indexed {file_path}")

    except Exception as e:
        print(f"WORKER: Error processing new file {file_path}: {e}")
        if manifest.data.get(str(file_path)):
            manifest.data[str(file_path)]["status"] = f"error: {e}"
            manifest.save()


def process_modified_file(
    file_path_str: str,
    manifest: "RAGManifest",
    rag_store: "RAGStore",
    embedder: "AtaraxAIEmbedder",
    chunk_size: int,
    chunk_overlap: int,
):
    file_path = Path(file_path_str)
    print(f"WORKER: Processing MODIFIED file: {file_path}")

    if not file_path.exists() or file_path.is_dir():
        print(
            f"WORKER: Modified file {file_path} not found (maybe deleted quickly). Treating as delete."
        )
        process_deleted_file(file_path_str, manifest, rag_store)
        return

    try:
        current_timestamp = file_path.stat().st_mtime
        current_hash = get_file_hash(file_path)
        if not current_hash:
            return

        manifest_entry = manifest.data.get(str(file_path))

        if manifest_entry and manifest_entry.get("hash") == current_hash:
            print(
                f"WORKER: File {file_path} content unchanged (hash match). Updating timestamp if newer."
            )
            if current_timestamp > manifest_entry.get("timestamp", 0):
                manifest_entry["timestamp"] = current_timestamp
                manifest.save()
            return

        print(
            f"WORKER: File {file_path} has changed or needs re-indexing. Re-processing..."
        )

        if manifest_entry and manifest_entry.get("chunk_ids"):
            print(f"WORKER: Deleting old chunks for {file_path} from RAG store.")
            rag_store.delete_by_ids(ids=manifest_entry["chunk_ids"])
            # Alternatively, or in addition, use metadata if IDs might change:
            # rag_store.delete_by_metadata(metadata_filter={"source_file": str(file_path)})

        # Re-process as a new file (this will update/overwrite manifest entry for this path)
        process_new_file(
            file_path_str, manifest, rag_store, embedder, chunk_size, chunk_overlap
        )

    except Exception as e:
        print(f"WORKER: Error processing modified file {file_path}: {e}")
        if manifest.data.get(str(file_path)):
            manifest.data[str(file_path)]["status"] = f"error: {e}"
            manifest.save()


def process_deleted_file(
    file_path_str: str, manifest: "RAGManifest", rag_store: "RAGStore"
):
    # file_path = Path(file_path_str) # File doesn't exist, so Path object is just for consistency in key
    print(f"WORKER: Processing DELETED file: {file_path_str}")

    try:
        manifest_entry = manifest.data.pop(
            str(file_path_str), None
        )  # Remove from manifest

        if manifest_entry and manifest_entry.get("chunk_ids"):
            print(f"WORKER: Deleting chunks for {file_path_str} from RAG store.")
            rag_store.delete_by_ids(ids=manifest_entry["chunk_ids"])
            # Or: rag_store.delete_by_metadata(metadata_filter={"source_file": file_path_str})
            manifest.save()  # Persist manifest changes
            print(f"WORKER: Successfully processed deletion of {file_path_str}")
        else:
            print(
                f"WORKER: File {file_path_str} not found in manifest or no chunk IDs to delete."
            )
    except Exception as e:
        print(f"WORKER: Error processing deleted file {file_path_str}: {e}")


# --- Worker Thread Function ---
def rag_update_worker(
    processing_queue: "queue.Queue",
    manifest: "RAGManifest",
    rag_store: "RAGStore",
    embedder: "AtaraxAIEmbedder",
    chunk_config: dict,
):
    """
    Worker function to process tasks from the queue.
    chunk_config: dict e.g., {"size": 400, "overlap": 50}
    """
    print(
        f"RAG Update Worker started. PID: {os.getpid()}, Thread: {threading.get_ident()}"
    )
    chunk_size = chunk_config.get("size", 400)
    chunk_overlap = chunk_config.get("overlap", 50)

    while True:
        try:
            task = processing_queue.get(timeout=1)
            if task is None:
                print("RAG Update Worker: Received sentinel. Exiting.")
                processing_queue.task_done()
                break

            print(f"RAG Update Worker: Got task {task}")
            event_type = task.get("event_type")
            file_path = task.get("path")
            dest_path = task.get("dest_path")

            if not file_path:
                print(f"RAG Update Worker: Invalid task, missing path: {task}")
                processing_queue.task_done()
                continue

            if event_type == "created":
                process_new_file(
                    file_path, manifest, rag_store, embedder, chunk_size, chunk_overlap
                )
            elif event_type == "modified":
                process_modified_file(
                    file_path, manifest, rag_store, embedder, chunk_size, chunk_overlap
                )
            elif event_type == "deleted":
                process_deleted_file(file_path, manifest, rag_store)
            elif event_type == "moved":
                if dest_path:
                    process_deleted_file(file_path, manifest, rag_store)
                    process_new_file(
                        dest_path,
                        manifest,
                        rag_store,
                        embedder,
                        chunk_size,
                        chunk_overlap,
                    )
                else:
                    print(
                        f"RAG Update Worker: Invalid 'moved' task, missing dest_path: {task}"
                    )
            else:
                print(
                    f"RAG Update Worker: Unknown event type '{event_type}' for task: {task}"
                )

        except queue.Empty:
            continue
        except Exception as e:
            print(
                f"RAG Update Worker: Unhandled error processing task {task if 'task' in locals() else 'UNKNOWN_TASK'}: {e}"
            )
        finally:
            if "task" in locals() and task is not None:
                processing_queue.task_done()
